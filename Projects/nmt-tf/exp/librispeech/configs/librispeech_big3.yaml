dirs:
    train:
        data: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/train+other.bpe.shuffle.txt
    dev:
        data: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/dev-clean/text
    test:
        data: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/test-clean/text

    type: scp
    models: models
    vocab: /mnt/lustre/xushuang/easton/projects/asr-tf/exp/librispeech/vocab_3724+1.txt
    log: log
    checkpoint: checkpoint
    # checkpoint_init: /mnt/lustre/xushuang/easton/projects/nmt-tf/exp/librispeech/models/librispeech_big.yaml/checkpoint/

data:
    num_steps: 35
    num_loops: 25
    unit: subword
    train:
        size_dataset:
    dev:
        size_dataset:

model:
    encoder:
        type: None
    decoder:
        type: SelfAttention
        num_blocks: 6
        num_cell_units: 512
        init_scale: 0.04
        attention_dropout_rate: 0.4
        residual_dropout_rate: 0.4
        num_heads: 8
        schedule: 0.8
        start_warmup_steps: 0
        step_increasement: 0.0
        softmax_temperature: 0.5
        size_embedding: 512
        share_embedding: False
    structure: languageModel

dev_step: 100
save_step: 100
keep_training:

# gpus: '1'
gpus: '0,1,2,3'

bucket_boundaries: 11,14,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,58,61,64,67,70,74,78,83,88,94,102,112,126,148,197,510,600

opti:
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-8

default_stddev: 0.046875

# learning rate
optimizer: adam
lr_type: warmup_exponential_decay
warmup_steps: 6000
peak: 0.0002
decay_steps: 10000
beam_size: 1
# beam_size: 10
num_threads: 8
num_batch_tokens: 20000

length_penalty_weight: 0.0
lamda_l2: 0.0

grad_clip_value: 0.0
slot_clip_value: 0.0
grad_clip_norm: 0.0
grad_clip_global_norm: 10.0
