dirs:
    train:
        data: /mnt/lustre/xushuang/easton/data/hkust/word/train/feats.train_3x.scp
        label: /mnt/lustre/xushuang/easton/data/hkust/word/train/word.train_3x.scp
        tfdata: /mnt/lustre/xushuang/easton/data/hkust/word_eos/train
    dev:
        data: /mnt/lustre/xushuang/easton/data/hkust/word/train_dev/feats.train_dev.scp
        label: /mnt/lustre/xushuang/easton/data/hkust/word/train_dev/word.train_dev.scp
        tfdata: /mnt/lustre/xushuang/easton/data/hkust/word_eos/train_dev
    test:
        data: /mnt/lustre/xushuang/easton/data/hkust/word/dev/feats.dev.scp
        label: /mnt/lustre/xushuang/easton/data/hkust/word/dev/word.dev.scp
        tfdata: /mnt/lustre/xushuang/easton/data/hkust/word_eos/dev
    type: scp
    models: models
    vocab: /mnt/lustre/xushuang/easton/data/hkust/word_eos/vocab_3673+2.txt
    log: log
    checkpoint: checkpoint
    # checkpoint_init: /mnt/lustre/xushuang/easton/projects/asr-tf/exp/hkust/models/transformer3.yaml/checkpoint

data:
    # dim_raw_input: 80
    num_context: 2
    downsample: 3
    add_delta: False
    input_noise: 0
    unit: word
    train:
        size_dataset:
    dev:
        size_dataset:

model:
    encoder:
        type: transformer_encoder
        num_blocks: 6
        num_heads: 16
        num_cell_units: 512
        activation: glu
        attention_dropout_rate: 0.0
        residual_dropout_rate: 0.3
    decoder:
        type: transformer_decoder
        size_embedding: 512
        num_blocks: 6
        num_heads: 16
        num_cell_units: 512
        activation: glu
        attention_dropout_rate: 0.0
        residual_dropout_rate: 0.3
        label_smoothing_confidence: 0.9
    structure: transformer
    training_type: teacher-forcing
    loss_type: CE
    prob_start: 0.1
    prob_end: 1.0
    start_warmup_steps: 8000
    interim_steps: 10000

dev_step: 1000
decode_step: 100
save_step: 1000

gpus: '0,1,2,3'
# gpus: '0'

num_epochs: 100000
keep_training: True
num_steps: 500000

bucket_boundaries: 40,53,62,69,76,82,88,93,98,103,108,113,118,122,127,133,138,144,150,156,163,171,180,190,203,218,240,274,336
num_batch_tokens: 16000
maxlen: 50

opti:
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-8

default_stddev: 0.046875

# learning rate
optimizer: adam
warmup_steps: 8000
peak: 0.0005
decay_steps: 8000
beam_size: 1
# beam_size: 10
num_threads: 8

length_penalty_weight: 0.0
lambda_l2: 0.00001
lambda_lm: 0.0
lambda_rerank: 0.0

grad_clip_value: 0.0
slot_clip_value: 0.0
grad_clip_norm: 0.0
grad_clip_global_norm: 5.0
