dirs:
    train:
        data: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/train/feats.scp
        label: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/train/text
        tfdata: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/train/tfdata_eos
    dev:
        data: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/dev-clean/feats.scp
        label: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/dev-clean/text
        tfdata: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/dev-clean/tfdata_eos
    test:
        data: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/test-clean/feats.scp
        label: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/test-clean/text
        tfdata: /mnt/lustre/xushuang/easton/data/libriSpeech/subword_data/data_word_piece/test-clean/tfdata_eos
    type: scp
    models: models
    vocab: /mnt/lustre/xushuang/easton/projects/asr-tf/exp/librispeech/vocab_3726.txt
    log: log
    checkpoint: checkpoint
    checkpoint_init: /mnt/lustre/xushuang/easton/projects/asr-tf/exp/librispeech/models/subwords_transformer.yaml//checkpoint/

data:
    dim_feature: 120
    num_context: 5
    downsample: 3
    add_delta: False
    unit: subword
    train:
        size_dataset:
    dev:
        size_dataset:

model:
    encoder:
        type: transformer_encoder
        num_blocks: 6
        num_heads: 8
        num_cell_units: 512
        attention_dropout_rate: 0.1
        residual_dropout_rate: 0.1
    decoder:
        type: transformer_decoder
        size_embedding: 512
        num_blocks: 6
        num_heads: 8
        num_cell_units: 512
        attention_dropout_rate: 0.1
        residual_dropout_rate: 0.1
        init_scale: 0.04
        label_smoothing_confidence: 0.9
    structure: transformer
    loss_type: CE
    prob_start: 0.1
    prob_end: 3.0
    start_warmup_steps: 8000
    interim_steps: 10000
    shallow_fusion: True
    rerank: True

dev_step: 500
decode_step: 50
save_step: 500

gpus: '0,1,2,3'

num_epochs: 10000
num_steps: 1000000
bucket_boundaries: 196,286,346,384,408,424,436,447,455,463,470,476,482,487,492,497,502,507,512,516,521,526,531,540,560,817
num_batch_tokens: 13000

opti:
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-8

default_stddev: 0.046875

# learning rate
optimizer: adam
## warm up need to be large enough!
warmup_steps: 15000
peak: 0.0002
decay_steps: 15000
beam_size: 1

length_penalty_weight: 0.0
lambda_l2: 0.0001

grad_clip_value: 0.0
slot_clip_value: 0.0
grad_clip_norm: 0.0
grad_clip_global_norm: 0.0
